---
title: "Designing a Tax Credit Policy for Work-Loss Days: A Predictive Modeling and Microsimulation Approach"
subtitle: "PPOL 6819 | Advanced Data Science | Final Project"
authors: "Jasmine Jha, Stephanie Jamilla, Maleeha Hameed"
format: html
editor: visual
embed-resources: true
editor_options: 
  chunk_output_type: console
bibliography: references.bib
---

## Preliminaries

```{r}

library(tidyverse)
library(tidymodels)
library(ggplot2)
library(haven)
library(here)
library(ipumsr)
library(janitor)
library(tictoc)
library(srvyr)
```

## Executive Summary

## 1. Background

### 1.1 Problem Statement

The United States is the only country in the OECD, and one of only six countries in the world, to not offer national paid parental leave. [@paidfam2022] Currently, 13 states and DC have mandatory paid family leave policies, all of which use social insurance models in which employers and employees pay into the program in order to access wages for leave.[@whatso2025] Discussion about paid family leave typically centers on parental leave, but these state policies also cover personal medical leave, allowing individuals to take care of their own serious health conditions. This is particularly important for individuals with chronic or serious health conditions who have to miss work and have no guarantee that their workplace will offer paid sick leave. Thus, such individuals may not earn any money for missed work days and thus have lower overall earnings compared to healthy individuals.

### 1.2 Policy Solution

This project proposes implementing a federal policy that mandates paid family leave, focusing on paid personal medical leave. The policy is based on current state policies--for example, states typically require that employees be granted up to 13 weeks of personal medical leave and have a maximum weekly benefit based on statewide average weekly wages. We create a baseline policy, then make adjustments to the dollar amount eligible employees receive or other components of the policy that determine the total amount transferred to individuals.

A primary concern about family leave policies is that they would adversely affect business productivity or profit as employees would take longer time off. However, a handful of studies on the effects of the policy in California and New York demonstrate that the policy has actually had a positive effect on employee productivity and no significant impact on employee performance.[@bartel2023][@bartel2023a]

## 2. Empirical Methodology

### 2.1 Data

#### Data Description

We used the 2018 dataset from the National Health Interview Survey, available on the IPUMS website. This dataset includes 72,831 observations and 1,503 variables. We selected the 2018 dataset because it offers a relatively large number of observations alongside a wide range of variables.

The 1503 variables pertained to household and individual level information on demographic characteristics, socioeconomic status, general health status, mortality, specific diseases, health conditions, health behaviours, health education, medical care and access, health insurance, mental health, vaccinations history, alternative health, and social determinants of health.

```{r}

#| output: false
#| warning: false
#| message: false

ddi <- read_ipums_ddi("data/nhis_00004.xml")
full_data <- read_ipums_micro(ddi)
```

#### Data Preprocessing

Our final analytic dataset consists of 72,752 observations and 104 variables. We applied several preprocessing steps to ensure the data was clean, interpretable, and ready for analysis and modeling.

```{r}

# Cleaning Names
full_data <- full_data |>
  clean_names()

# Selecting 104 variables
work_loss <- full_data |>
  select(nhispid, age, sex, sexorien, marstleg, relate, famsize, nchild, nsibs, famkidno, famoldno, parenthere, racenew, hispeth, yrsinusg, usborn, citizen, regionbr, intervlang, educ, headstarev, headstarnow, empstat, occ, ind, hourswrk, secondjob, paidhour, paidsick, empstatwkyr, classwk2, jobsecincorp, workev, yearsonjob, mainlong, whynowk2, pooryn, incfam07on, earnings, welfmo, gotwelf, gotnewelf, gotssi, gotssiwhy, ssiapply, gotss, gotnonssdis, ssdiapply, gotdiv, gotint, gotothpen, gotwage, gotsemp, gotchsup, gotwic, wiceligible, wicflag, gotstampfam, stampmo, gotother, poverty, fsateless, fsbalanc, fshungry, fsnoteat, fsnoteatno, fsnotlast, fsrawscore, fsrunout, fsskip, fsskipno, fsstat, fsweight, ownership, lowrent, health, bmi, bedayr, hstatyr, wldayr, workvolhcset, workvolhcpat, nbhdtime, pregnantnow, pregflush, hystev, bwgtgram, dvint, docvis2w, homecare2w, phonemed, famcare10xno, mamnoflup, mamother, mamult, papev, papab3yr,psalest, psaly, psasugg, mortelig, mortstat, mortdody, mortucodld)


# Recoding for NA values - there were no values to change
work_loss <- work_loss |>
  mutate(famsize = case_when(
    famsize == 99 ~ 0,
    famsize == 98 ~ NA_real_,
    TRUE ~ famsize
  ))

# Recoding for NA values 
work_loss <- work_loss |>
  mutate(across(c(famkidno, famoldno, hourswrk, welfmo), ~ifelse(. == 98, NA, .)))

work_loss <- work_loss |>
  mutate(across(c(hourswrk, welfmo), ~ifelse(. %in% c(97, 99), NA, .)))

work_loss <- work_loss |>
  mutate(across(c(bedayr, wldayr), ~ifelse(. %in% c(997, 998, 999), NA, .)))

work_loss <- work_loss |>
  mutate(bedayr = ifelse(bedayr == 996, 0, bedayr),
         wldayr = ifelse(wldayr == 996, 0, wldayr))

work_loss <- work_loss |>
  mutate(bwgtgram = ifelse(bwgtgram %in% c(9997, 9998, 9999), NA, bwgtgram))

work_loss <- work_loss |>
  mutate(famcare10xno = ifelse(famcare10xno == 9, NA, famcare10xno))

# Transform all categorical variables to factor
## Creating custom function
make_factor <- function(variable) {
    
  work_loss |>
    mutate(across(c(variable), as.factor))
  
}

## Creating a list of all categorical variable names
vars <- select(.data = work_loss, -nhispid, -age, -famsize, -famkidno, -famoldno, -hourswrk, -welfmo, -bmi, -bedayr, -wldayr, -bwgtgram, -famcare10xno)
list <- (colnames(vars))

## Iterating the function over each categorical variable
work_loss <- make_factor(variable = list)

## Checking to see if variable type is correct (most variables should be <fct> type now)
glimpse(work_loss)

```

1.  **Standardization of variable names:** We standardized all variable names to ensure consistency and facilitate downstream processing. This involved converting all names to lowercase and replacing spaces and special characters with underscores. These modifications improved both readability and compatibility with data manipulation functions.

2.  **Variable selection:** We manually reviewed and filtered the original set of 1,503 variables, reducing it to a focused subset of 104 variables. This selection excluded any variables with imputations, flags, or duplicate variants (e.g., summary or imputed versions of the same item). The final dataset includes variables from the following domains:

    -   **Demographics**: age, sex, marital status
    -   **Family composition**: household size, number of children, number of older adults
    -   **Employment and income**: job type, hours worked, earnings, welfare receipt
    -   **Health**: body mass index (BMI), days in bed due to illness, caregiving frequency

    This streamlined selection was guided by theory and relevance to the research question.

3.  **Recoding special and missing values:** We cleaned the data by identifying and recoding special response codes that represented missing or invalid data. For example, values such as 98, 99, and 997--999 were treated as missing, while specific codes like 996 (indicating "none" or "zero") were recoded to 0. This step ensured that all numeric fields reflected interpretable and analyzable values. Additionally, this step ensured numerical consistency and prevented skewed analysis due to placeholder codes.

4.  **Conversion of categorical variables:** We converted all applicable non-numeric variables to categorical format to support appropriate statistical analysis and visualization. This included responses related to employment status, education, language, and benefit receipt. This step was essential for accurate modeling and visualization, particularly for tree-based and regularized regression models.

5.  **Identification of the outcome variable:** Our primary outcome variable is **`wldayr`**, representing the number of work-loss days in the past 12 months.

### 2.2 Approaches

We use two complementary machine learning approaches to explore the policy question at hand: predictive modeling and microsimulation.

#### Predictive Modeling

The first part of the project focuses on developing two predictive models to estimate the number of work-loss days an individual may experience, using features such as demographics, health history, insurance coverage, and other socioeconomic factors. These models aim to help policymakers understand how illness and injury impact labor force participation, thereby informing the design of targeted policy interventions.

We employ two modeling techniques---random forests (a tree-based ensemble method) and elastic net regression (a regularized linear model that balances L1 and L2 penalties). These methods were chosen for their complementary strengths: random forests are capable of capturing nonlinear interactions and complex variable relationships, while elastic net regression offers interpretability and variable selection in high-dimensional settings. We compare their performance to identify the most effective and interpretable model for informing policy design. Additional methodological details are provided in Section 3.1.

#### Microsimulation

The second part of the project involves the creation of a microsimulation using a built-in microsimulation calculator to implement a proposed tax credit or cash transfer policy.

We begin by defining the policy parameters, including how the tax credit amount is determined based on the number of work-loss days, household income, number of children, and other relevant factors. The microsimulation allows us to estimate the impact of the policy at the individual and household level, simulating outcomes under different eligibility rules and benefit structures.

In the third step, we systematically vary key parameters of the policy to assess the distributional effects of these changes. This sensitivity analysis helps identify equity concerns and possible unintended consequences such as employers reducing paid sick leave offerings in response to the new government provision. To contextualize these concerns, we draw on existing literature from the U.S. and other countries regarding employer responses to publicly funded sick leave programs.

### 2.3 Limitations

-   **Arbitrary variable selection:** We reduced the raw dataset to 104 variables based on a combination of intuition and theoretical considerations, excluding variables that were repetitive, imputed, or had excessive missing values. While a more principled, data-driven variable selection approach may have enhanced the predictive performance and accuracy of our microsimulation models, the full set of 1,503 variables was too unwieldy for the methods employed in this project.

-   **Limited number of predictive models:** Although we would have preferred to explore a broader set of predictive models, such as XGBoost or Generalized Additive Models (GAMs), and experiment with hyperparameter tuning, computational constraints limited our ability to do so. These models, while potentially more powerful, were too resource-intensive given our available infrastructure.

-   **Lack of geographical data:** Incorporating geographic data would have enabled us to distinguish between state and federal policy effects in our microsimulation calculator, providing a more nuanced understanding of policy implementation. Unfortunately, the absence of such data restricted our ability to analyze these geographic variations.

## 3. Predictive Modelling

### 3.1 Setup

#### 3.1.1 Split data into testing and training data

```{r}

# Split the dataset
set.seed(2025)

work_loss_split <- initial_split(work_loss, prop = 0.8)

work_loss_train <- training(x = work_loss_split)
work_loss_test  <- testing(x = work_loss_split)
```

#### 3.1.2 Set up resampling for model selection

```{r}

# Set crossvalidation
work_loss_folds <- vfold_cv(data = work_loss_train, v = 10)
```

### 3.2 Create Candidate Models

#### 3.2.1 Random Forests

Random Forests is a tree-based model that uses bagging, which is an ensembling method. The model considers random sections of the dataset and a subset of predictors to train each regression tree. This method was chosen based on past experience with this nonparametric model performing well (i.e., achieving a relatively low RMSE value) compared to other models.

However, one downside is that random forests is not as easily interpretable compared to other models, such as linear regression that can offer clearer coefficients and directions between the predictors and the outcome variable. It is not easy to understand how the random forest model makes predictions across the dataset and its element of randomness further complicates interpretability.

```{r}

# Create recipe
rf_rec <- recipe(formula = wldayr ~ ., data = work_loss_train) |>
  update_role(c(nhispid), new_role = "id") |>
  step_dummy(all_factor_predictors()) |>
  step_rm(has_role("id")) |>
  step_zv(all_predictors()) |>
  step_corr(all_numeric_predictors()) |>
  step_impute_mean(all_numeric_predictors()) |>
  step_naomit(wldayr)

# Create model
rf_mod <- rand_forest(
  trees = 200,
  mtry = 15,  
  min_n = tune()
) |>
  set_mode(mode = "regression") |>
  set_engine(
    engine = "ranger",
    importance = "impurity",
    num.threads = 4
  )

# Create workflow
rf_wf <- workflow() |>
  add_recipe(rf_rec) |>
  add_model(rf_mod)

# Create grid
rf_grid <- grid_regular(
  min_n(range = c(1, 15)),
  mtry(range = c(1, 15)),
  levels = 5
)

# Tune grid
rf_tuning <- tune_grid(
  rf_wf,
  resamples = work_loss_folds,
  grid = rf_grid
)

# Show best tuning results
show_best(rf_resamples)

```

```{r}

# Visualize tuning results
autoplot(rf_tuning)
```

```{r}

# Finalize workflow
rf_tuned_wf <-
  rf_wf |>
  tune::finalize_workflow(tune::select_best(x = rf_tuning, metric = "rmse"))

# Fit final workflow
rf_resamples <- rf_tuned_wf |>
  fit_resamples(resamples = work_loss_folds) 

# Collect metrics
rf_resamples |>
  collect_metrics()

```

```{r}

rf_rmse_plot <- collect_metrics(
  rf_resamples, 
  summarize = FALSE) |>
  filter(.metric == "rmse") |>
  ggplot(mapping = aes(x = id, y = .estimate, group = .metric)) +
  geom_point() +
  geom_line() +
  theme_void() +
  labs(
    title = "RMSE of Random Forests Model Fits Across Folds") +
  xlab("Fold") +
  ylab("RMSE")

rf_rmse_plot
```

#### 3.2.1 Elastic Net Regression

This model uses regularization, which is a powerful technique for parametric regression models. Regularization, or penalization, allows linear regression to work with very wide data, to generate stable estimates for data with multicollinearity, and to perform feature selection.

Elastic net regression combines ridge regression and LASSO regression. It has two hyperparameters, λ1 and λ2. Sometimes the hyperparameters are λ and mixture, which determines how much of λ to apply to each penalty (i.e. mixture = 0 is ridge regression and mixture = 1 is LASSO regression).

We used this model as it can work with datasets with a large number of variables, generate stable esitmates for data with multicollinearity, perform feature selection, but in a less dramatic fashion than LASSO regression

Elastic net regression requires all variables to be centered and scaled (standardized) before estimation. This way, the coefficients are interpreted in standardized units. To meet this criteria, we created a recipe that removed an ID variables, removed correlated numerical predictors, imputed the mean for any numerical predictors with missing values, hot coded binary variables for categorical ones, removed any predictors with zero variance or near-zero variance. Additionally, we removed missing observations in the outcome variable.

```{r}

# Create recipe
enet_rec <- recipe(wldayr ~ ., data = work_loss_train) |>
  update_role(c(nhispid), new_role = "ID") |>
  step_rm(has_role("ID")) |>
  step_dummy(all_factor_predictors()) |>
  step_zv(all_predictors()) |>
  step_corr(all_numeric_predictors()) |>
  step_impute_mean(all_numeric_predictors()) |>
  step_naomit(wldayr)

# Create model
enet_mod <- linear_reg(penalty = tune(), mixture = tune()) |>
  set_mode(mode = "regression") |>
  set_engine(engine = "glmnet")  

# Create workflow
enet_wf <- workflow() |>
  add_recipe(recipe = enet_rec) |>
  add_model(spec = enet_mod)

# Create grid
enet_grid <- grid_regular(
  penalty(), 
  mixture(),
  levels = 20
)

# Tune grid
enet_tuning <- tune_grid( 
  enet_wf,
  resamples = work_loss_folds, 
  grid = enet_grid
)

# Identify best tuning results
show_best(enet_tuning)
```

```{r}

# Visualize tuning results
autoplot(enet_tuning)
```

The RMSE calculated is 8.17. This shows that regularization does not help much in this case. This makes sense because the regression specification is parsimonious and deliberate.

```{r}

# Finalize workflow
enet_tuned_wf <-
  enet_wf |>
  tune::finalize_workflow(tune::select_best(x = enet_tuning, metric = "rmse"))

# Fit final workflow
enet_resamples <- enet_tuned_wf |>
  fit_resamples(resamples = work_loss_folds) 

# Collect metrics
enet_resamples |>
  collect_metrics()
```

```{r}

enet_rmse_plot <- collect_metrics(
  enet_resamples, 
  summarize = FALSE) |>
  filter(.metric == "rmse") |>
  ggplot(mapping = aes(x = id, y = .estimate, group = .metric)) +
  geom_point() +
  geom_line() +
  theme_void() +
  labs(
    title = "RMSE of Elastic Net Regression Model Fits Across Folds") +
  xlab("Fold") +
  ylab("RMSE")

enet_rmse_plot
```

### 3.3 Model Comparison

```{r}

# Compile results from both models

bind_rows(
  "Random forest" = collect_metrics(rf_resamples) |> filter(.metric == "rmse"),
  "Elastic net regression" = collect_metrics(enet_resamples) |> filter(.metric == "rmse"),
  .id = "model"
)
```

```{r}

# Select the best resamples via select_best
model_best <- rf_resamples |>                    
  select_best(metric = "rmse")

# Finalize the workflow with finalize_workflow
model_final <- finalize_workflow(
  rf_workflow,                                   
  parameters = model_best
)

# Fit the model on all the data
model_final_fit <- 
  model_final |>
  last_fit(work_loss_split) 

# Collect metrics for RMSE
kable(model_final_fit |>
  collect_metrics() |>
  filter(.metric == "rmse"))
```

```{r}

# Make predictions with the testing data
model_predictions <- bind_cols(
  work_loss_test,
  predict(object = extract_workflow(model_final_fit), 
          new_data = work_loss_test) 
)

# Display the first 10 predictions against the actual values
kable(select(model_predictions, wldayr, starts_with(".pred")))
```

```{r}

# Interrogate which values were incorrectly predicted to the nearest tenth

kable(model_predictions |>
  select(wldayr, .pred) |>
  mutate(.pred = round(model_predictions$.pred, 1),
         wldayr = round(model_predictions$wldayr, 1),
         correct = if_else(.pred == wldayr, "Yes", "No"
         )) |>
  count(correct))
```

### 3.4 Variable Importance

```{r}

model_final_fit |>
  extract_fit_parsnip() |>
  vip(num_features = 20) +
  labs(title = "Top Predictors in Best Performing Model") +
  ylab("Importance") +
  xlab("Variable")
```

## 4. Microsimulation

We developed a microsimulation using a nationally representative dataset to evaluate the impact of a proposed cash transfer policy aimed at supporting individuals experiencing work loss. The core objective is to translate the policy's eligibility criteria and benefit formula into individual-level estimates, allowing us to assess who benefits, by how much, and how the benefits are distributed across income groups.

The simulation begins by defining key policy parameters:

1.  The threshold of work loss days required for eligibility

2.  The reimbursement rate per week or day

3.  Any caps on total reimbursement

4.  Income-based or poverty-based eligibility conditions

We then implement these rules to each individual wihin the full data set and generate estimates of their potential reimbursement under different policy designs. By weighting these estimates, we can calculate population-level outcomes such as the mean reimbursement and the number of eligible individuals, as well as explore differences by income group.

```{r}

# For microsimulation
full_data <- full_data |>
  filter(!wldayr %in% c(996,997, 998, 999))
```

### **4.1 Counterfactual**

The first simulation represents a counterfactual scenario with broad eligibility, essentially modelling what reimbursement would look like under a policy where all are eligible:

1.  Eligibility: Anyone with more than 0 work loss day is eligible for reimbursement.

2.  Benefit formula:

    -   Individuals with less than or equal to 84 work loss days receive a reimbursement proportional to their number of lost days:

    -   Reimbursement equals the number of work loss days multiplied by the weekly reimbursement rate divided by 7.

    -   Individuals with greater than 84 work loss days receive a capped reimbursement equal to the full rate for 12 weeks.

This simulation calculates reimbursement amounts for every individual, regardless of income or poverty status, and uses survey weights (PERWEIGHT) to estimate the mean reimbursement and the total eligible population.

```{r}
simulate_reimbursement_cf <- function(wldayr, poverty, perweight, reimbursement_rate, num_weeks) {
  
  eligible <- (wldayr > 0)
  
  reimbursement <- numeric(length(wldayr))
  
  below <- eligible & (wldayr <= 84)
  above <- eligible & (wldayr > 84)
  
  reimbursement[below] <- (wldayr[below]) * (reimbursement_rate / 7)
  reimbursement[above] <- reimbursement_rate * num_weeks
  
  data.frame(
    id = seq_along(wldayr),
    reimbursement = reimbursement,
    perweight = perweight,
    income = full_data$incfam07on
  )
}

```

```{r}
simulated_data_cf <- simulate_reimbursement_cf(
  wldayr = full_data$wldayr,
  poverty = full_data$poverty,
  perweight = full_data$perweight,
  reimbursement_rate = 1194,
  num_weeks = 12
)


weighted_data_cf <- simulated_data_cf |> 
  as_survey_design(weights = perweight)


result_cf <- weighted_data_cf |>
  summarise(
    mean_reimbursement = survey_mean(reimbursement),
    num_eligible = survey_total(as.numeric(reimbursement > 0))
  )

# Graph to show mean reimbursement per family income level
simulated_data_cf |>
  group_by(income) |>
  filter(income < 96 & income != 10 & income != 21) |>
  mutate(mean_reimbursement = mean(reimbursement),
         income = case_when(
           income == 11 ~ "$0 - $34,999",
           income == 12 ~ "$35,000 - $49,999",
           income == 22 ~ "$50,000 - $74,999",
           income == 23 ~ "$75,000 - $99,999",
           income == 24 ~ "$100,000 and over"
         ),
    income = factor(income, levels = c("$0 - $34,999", "$35,000 - $49,999", "$50,000 - $74,999", "$75,000 - $99,999", "$100,000 and over"))) |>
  count(mean_reimbursement) |>
  ggplot() +
  geom_col(aes(x = income, y = mean_reimbursement, fill = income)) +
  labs(title = "Average reimbursement amount per income level",
      x = "Income Level",
      y = "Mean Reimbursement") +
  theme_minimal() +
  theme(legend.position = "none")


```

The bar chart above shows the average reimbursement per income level. This is under the simulated counter-factual reimbursement model.

We see that people earning between \$0 to \$34999 and \$3500 to \$49999 receive the highest average reimbursement of about \$700. People earning \$100000 and over received the lowest reimbursement of about \$500. Under this policy, lower income groups received higher average reimbursement than higher income, despite equal reimbursement caps. This reflects that lower-income workers had more work-loss days.

### **4.2 Adjusted Policy Model**

The second simulation implements a targeted policy with stricter eligibility rules, reflecting a more realistic intervention:

1.  Eligibility:

    -   Individuals must have at least 28 work loss days

    -   Must fall below a specified poverty threshold (poverty index less than 37 or equal to 38).

2.  Benefit formula:

    -   Eligible individuals with less than or equal to 91 work loss days are reimbursed only for days beyond the 28-day threshold

    -   Reimbursement=(work loss days−28)×(reimbursement rate / 7​)

    -   Individuals with greater than 91 work loss days receive a capped reimbursement equal to the rate for 9 weeks.

As in the counterfactual, we estimate mean reimbursement and number eligible using survey weights (PERWEIGHT), and visualize results by income level.

```{r}
simulate_reimbursement <- function(wldayr, poverty, perweight, reimbursement_rate, num_weeks) {
  eligible <- (wldayr > 28 & (poverty < 37 | poverty == 38))  
  
  reimbursement <- numeric(length(wldayr))
  
  below <- eligible & (wldayr <= 91)
  above <- eligible & (wldayr > 91)
  
  reimbursement[below] <- (wldayr[below] - 28) * (reimbursement_rate / 7)
  reimbursement[above] <- reimbursement_rate * num_weeks
  
  data.frame(
    id = seq_along(wldayr),
    reimbursement = reimbursement,
    perweight = perweight,
    income = full_data$incfam07on
  )
}

```

```{r}
simulated_data_custom_rate <- simulate_reimbursement(
  wldayr = full_data$wldayr, 
  poverty = full_data$poverty, 
  perweight = full_data$perweight,
  reimbursement_rate = 1194,
  num_weeks = 9
)

weighted_data <- simulated_data_custom_rate |> 
  as_survey_design(weights = perweight)


result_final <- weighted_data |>
  summarise(
    mean_reimbursement = survey_mean(reimbursement),
    num_eligible = survey_total(as.numeric(reimbursement > 0))
  )

# Graph to show mean reimbursement per family income level
simulated_data_custom_rate |>
  group_by(income) |>
  filter(income < 96 & income != 10 & income != 21) |>
  mutate(mean_reimbursement = mean(reimbursement),
         income = case_when(
           income == 11 ~ "$0 - $34,999",
           income == 12 ~ "$35,000 - $49,999",
           income == 22 ~ "$50,000 - $74,999",
           income == 23 ~ "$75,000 - $99,999",
           income == 24 ~ "$100,000 and over"
         ),
    income = factor(income, levels = c("$0 - $34,999", "$35,000 - $49,999", "$50,000 - $74,999", "$75,000 - $99,999", "$100,000 and over"))) |>
  count(mean_reimbursement) |>
  ggplot() +
  geom_col(aes(x = income, y = mean_reimbursement, fill = income)) +
  labs(
    title = "Average reimbursement amount per income level",
    x = "Income Level",
    y = "Mean Reimbursement"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

```

In this case the reimbursements cap is lowered from 12 weeks to 9 weeks. Eligibility is also narrowed based on poverty score. People in income level \$0 to \$34999 have the highest mean reimbursement of about \$240. and people in income level \$100000 and over have the lowest mean reimbursement of about \$30.

Lower income group still has the high average reimbursement but it is lower than the counterfactual model. This adjusted policy model targets reimbursements towards lower-income individuals by incurring lower total expenditure by the government.

### **4.3 Comparison of Microsimulation Models**

The key difference between the models is in the way we define eligibility and by how much they receive.

The counterfactual model is universal, it includes anyone with any work loss, regardless of poverty level or severity, resulting in a larger eligible population and higher overall expenditure. On the other hand, the adjusted policy model is targeted, it restricts benefits to lower-income individuals who have experienced substantial work loss, leading to a smaller eligible group and lower total reimbursement.

Both models provide insights into the distributional effects of the policy. While the counterfactual shows the maximum reach of benefits, the adjusted policy model reveals how narrowing eligibility shifts support toward the most disadvantaged but reduces total expenditure.

```{r}
simulated_data_cf$model <- "Counterfactual"
simulated_data_custom_rate$model <- "Adjusted Policy"

comparison_data <- rbind(simulated_data_cf, simulated_data_custom_rate)

comparison_data <- comparison_data |>
  select(reimbursement, model)

summary_stats <- comparison_data |>
  group_by(model) |>
  summarise(
    mean_reimbursement = mean(reimbursement),
    sd_reimbursement = sd(reimbursement),
    min_reimbursement = min(reimbursement),
    max_reimbursement = max(reimbursement)
  )


ggplot(comparison_data, aes(x = reimbursement, color = model)) +
  geom_line(stat = "density", size = 1.5) + 
  scale_color_manual(values = c("blue", "red")) +  
  labs(
    title = "Comparison of Reimbursements: Counterfactual vs. Adjusted Policy",
    x = "Reimbursement Amount",
    y = "Density",
    color = "Model"
  ) +
  theme_minimal() + 
  theme(
    legend.position = "top", 
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold")  
  )

```

The graph above, compares the distribution of reimbursement under the counter-factual and adjusted policy model.

Under the adjusted policy model the curve (blue) is slightly leftward shifted as compared to the counter-factual (red). This reflects the lower reimbursement amounts overall. Although lower income group still receive a higher reimbursement, it is comparatively lower in adjusted policy model. This is due to eligibility and lower number of weeks being reimbursed. The adjusted policy model achieves greater targeting of benefits towards lower-income individuals while reducing the total government expenditure.

## 5. Conclusion

This study has important implications for policymakers in their decision-making process regarding predictive model selection and policy implementation. Policymakers must weigh factors such as the computational power and interpretability of different predictive models. For example, selecting a more sophisticated model may provide greater accuracy, but it could be more computationally intensive, requiring trade-offs between resource allocation and predictive precision.

When considering which version of a tax credit disbursement policy to implement, policymakers should focus on the broader societal impact, balancing economic efficiency against the need for equitable outcomes. A policy that is highly efficient may not always address the needs of the most vulnerable populations, so equity considerations should be integrated into the decision-making process, ensuring that the benefits of the policy are distributed fairly.

Looking to the future, there are several avenues to further enhance our analysis. With more time and processing power, incorporating a wider range of variables into the predictive model could improve accuracy and offer deeper insights. Additionally, expanding the microsimulation by integrating additional datasets that offer more granular geographic information would enable us to explore the effects of state-level policies on federal policy implementation, helping to better understand the nuances of policy interactions across different levels of government.

## 6. References

Bartel, A. (2023, September 25). Getting the Job Done: How Paid Family Leave Impacts Employers. Columbia Business School. https://business.columbia.edu/research-brief/research-brief/getting-job-done-how-paid-family-leave-impacts-employers

Bartel, A. (2023, September 25). Getting the Job Done: How Paid Family Leave Impacts Employers. Columbia Business School. https://business.columbia.edu/research-brief/research-brief/getting-job-done-how-paid-family-leave-impacts-employers

Davis, J. (2025, April 8). States with paid family leave 2025: Map with employer requirements. Onpay. https://onpay.com/insights/paid-family-leave-by-state/

Paid Family Leave Across OECD Countries. (2022, March 1). Bipartisan Policy Center. https://bipartisanpolicy.org/explainer/paid-family-leave-across-oecd-countries/

Usual Weekly Earnings Summary. (2025, April 16). U.S. Bureau of Labor Statistics. https://www.bls.gov/news.release/wkyeng.nr0.htm

What's on the Table? A Review of Federal Paid Family Leave Proposals. (2025, April 28). Bipartisan Policy Center. https://bipartisanpolicy.org/explainer/whats-on-the-table-a-review-of-federal-paid-family-leave-proposals/
